/* Title: Reading Large Kaggle Datasets With PySpark in Google Colab / Initializing PySpark in Google Colab */
/* About: Install Pyspark and use it remotely on Google Colabs with a few lines of code */
/* Note: All setup codes below need to be run EACH TIME you want to run PySpark in Google Colab */
/* Requires own Kaggle API Token: Account >  API > Create new API. A file named kaggle.json is automatically downloaded. Mount to GDRIVE */



/* 1) Get data from Kaggle by inputting the following codes below */

/* Description: Installs the Kaggle library to be able to use the Kaggle API
   Creates a directory/folder called .kaggle in the root directory
   Copies the kaggle.json saved in personal g-drive to the new directory created above. */

! pip install kaggle
! mkdir ~/.kaggle
! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

/* Copy API Command from each dataset and replace line 18 as: ! {input API command} */

! kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store
! unzip ecommerce-behavior-data-from-multi-category-store.zip


/* 2) Install PySpark dependencies */
/* Description: Dependencies include Java 8, Apache spark with Hadoop, and FindSpark */

! apt-get install openjdk-8-jdk-headless -qq > /dev/null
! wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
! tar xf spark-3.1.1-bin-hadoop3.2.tgz
! pip install -q findspark


/* 3) Next is to set the environment path that enables us to run PySpark in our Colab environment by setting the location of Java and Spark */

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"


/* 4) Finally, run a local spark session to test installation  */

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

/* Ready to start analyzing table in PySpark! */
